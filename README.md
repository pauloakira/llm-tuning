# llm-tuning

The main goal of this repository is to test Large Language Model locally and benchmark them. Two metrics are being developed in here to compare the performance of different decoder-based LLM . The first metric consists in a statistics based score that uses the Levenstein Distance to compute a score for each response generated by the model regarding to manufactured tasks. The second metric simply checks if the model's respons contains the correct answer for the manufactured tests. To log the performance of each model, a MLFlow pipeline is implemented. To check the pipeline loggings, one shall run the following command:

```
mlflow ui
```

## Foundation Models
Basically the models that runs in this repository uses the HuggingFace Transformer library. Currently the models ready for use are:

1. [GPT Neo](https://huggingface.co/docs/transformers/en/model_doc/gpt_neo)
2. [Falcon-1B](https://huggingface.co/tiiuae/falcon-rw-1b)
3. [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b)
4. [Mistral-7B](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)

Also, we are using the Apple's MLX to run some models:

1. [Mistral-7B](https://huggingface.co/mlx-community/Mistral-7B-Instruct-v0.2)
2. [Gemma-2B](https://huggingface.co/mlx-community/quantized-gemma-2b-it)
3. [Qwen1.5-0.5B](https://huggingface.co/mlx-community/Qwen1.5-0.5B-Chat)